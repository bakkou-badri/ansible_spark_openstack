---
# Spark stuff
- block: 
  - name: download spark
    get_url:
      url=http://mirror.easyname.ch/apache/spark/spark-{{spark_version}}/spark-{{spark_version}}-bin-hadoop2.6.tgz  
      dest=/usr/local/spark-{{spark_version}}-bin-hadoop2.6.tgz

  - name: unzip spark
    unarchive: 
      copy=no 
      src=/usr/local/spark-{{spark_version}}-bin-hadoop2.6.tgz 
      dest=/usr/local/
      owner={{ hadoop_user }}
      group=hadoop 
      creates=/usr/local/spark-{{spark_version}}-bin-hadoop2
    
  - name: make spark symlink
    file: 
      src=/usr/local/spark-{{spark_version}}-bin-hadoop2.6 
      dest=/usr/local/spark state=link 
      owner={{ hadoop_user }}
      group=hadoop 

  tags:
    - downloads

- block: 
  - name: deploy slaves configuration
    template: src=templates/slaves.j2 dest=/usr/local/spark/conf/slaves 

  - name: deploy spark-env.sh configuration
    template: src=templates/spark-env.sh.j2 dest=/usr/local/spark/conf/spark-env.sh owner={{ hadoop_user }} group=hadoop
  
  tags: 
    - configuration

- name: install findspark
  pip: name=findspark state=present

- name: add spark environment variables to bashrc
  lineinfile:
    dest=~/.bashrc 
    state=present insertafter=EOF 
    line="export SPARK_HOME=/usr/local/spark; export PYSPARK_PYTHON=/usr/local/miniconda/bin/python"
    create=true
  with_items: 
    - "{{ core_user_list }}"
    - "{{ users }}"
    - "root"
  