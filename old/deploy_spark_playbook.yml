---
# ------------------------
# Deploy the general stuff
# ------------------------
- hosts: all
  sudo: yes

  vars:
    ssh_public_key_content: "{{ lookup('file', ssh_public_key ) }}"
  
  vars_files:
    - vars/main.yml
 
  pre_tasks:
    - name: Update APT cache
      apt: update_cache=yes

  tasks:

    # General tasks
    - block:
      - name: install java
        apt: name=openjdk-7-jre state=present update_cache=yes

      - name: disable net.ipv6.conf.all.disable_ipv6
        sysctl: name=net.ipv6.conf.all.disable_ipv6 value=1 state=present

      - name: disable net.ipv6.conf.default.disable_ipv6
        sysctl: name=net.ipv6.conf.default.disable_ipv6 value=1 state=present
 
      - name: disable net.ipv6.conf.lo.disable_ipv6
        sysctl: name=net.ipv6.conf.lo.disable_ipv6 value=1 state=present

      - name: distribute host file
        template: src=templates/hosts.j2 dest=/etc/hosts

      tags:
        - general

    # Install hadoop
    - block:
      - name: create hadoop group
        group: name=hadoop state=present

      - name: create hadoop user
        user: name={{ hadoop_user }} comment="Hadoop user" group=hadoop shell=/bin/bash

      - name: Get hadoop 
        get_url:
          url=http://mirror.switch.ch/mirror/apache/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz 
          dest=/usr/local/hadoop-2.7.2.tar.gz

      - name: unzip hadoop
        unarchive: 
          copy=no 
          src=/usr/local/hadoop-2.7.2.tar.gz 
          dest=/usr/local/
          owner={{ hadoop_user }} 
          group=hadoop
          creates=/usr/local/hadoop-2.7.2

      - name: set user and priviliges on hadoop
        file: path=/usr/local/hadoop-2.7.2 owner={{ hadoop_user }} group=hadoop recurse=yes

      - name: make hadoop symlink
        file: src=/usr/local/hadoop-2.7.2 dest=/usr/local/hadoop owner={{ hadoop_user }} group=hadoop state=link

      tags: 
        - downloads

    - block: 
      - name: distribute hadoop conf
        template: src=templates/hadoop-env.sh.j2 dest=/usr/local/hadoop/etc/hadoop/hadoop-env.sh

      - name: distribute hadoop core-site.xml
        template: src=templates/core-site.xml.j2 dest=/usr/local/hadoop/etc/hadoop/core-site.xml

      - name: distribute hadoop hdfs-site.xml
        template: src=templates/hdfs-site.xml.j2 dest=/usr/local/hadoop/etc/hadoop/hdfs-site.xml

      - name: distribute hadoop masters file
        template: src=templates/masters.j2 dest=/usr/local/hadoop/etc/hadoop/masters
      
      - name: distribute hadoop slaves file
        template: src=templates/slaves.j2 dest=/usr/local/hadoop/etc/hadoop/slaves
      
      tags:
        - configuration

    # Setup ssh keys and user accounts
    - block: 
      - name: deploy authorized keys
        authorized_key: user={{ item }} key="{{ ssh_public_key_content }}"
        with_items:
          - "{{ user }}"
          - "{{ hadoop_user }}"

      - name: deploy ssh-keys
        copy: src={{ssh_keys_to_use}} dest=/home/{{ item }}/.ssh/
        with_items:
          - "{{ user }}"
          - "{{ hadoop_user }}"

      - name: distribute ssh config
        template: src=templates/config.j2 dest=/home/{{ item }}/.ssh/config
        with_items:
          - "{{ user }}"
          - "{{ hadoop_user }}"

      - name: add hadoop and spark binaries to path for hadoop user
        lineinfile:
          dest=/home/{{ item }}/.bashrc 
          state=present insertafter=EOF 
          line="export PATH=$PATH:/usr/local/hadoop/bin/:/usr/local/spark/bin/"
          create=true
        with_items: "{{core_user_list}}"
      
      #############
      # TO-DO: create user accounts and inject other keys
      #############

      tags:
        - user-accounts

    # Spark stuff
    - block: 
      - name: download spark
        get_url:
          url=http://mirror.easyname.ch/apache/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz  
          dest=/usr/local/spark-1.6.1-bin-hadoop2.6.tgz

      - name: unzip spark
        unarchive: 
          copy=no 
          src=/usr/local/spark-1.6.1-bin-hadoop2.6.tgz 
          dest=/usr/local/
          owner={{ hadoop_user }}
          group=hadoop 
        
      - name: make spark symlink
        file: 
          src=/usr/local/spark-1.6.1-bin-hadoop2.6 
          dest=/usr/local/spark state=link 
          owner={{ hadoop_user }}
          group=hadoop 

      tags:
        - downloads

    - block: 
      - name: deploy slaves configuration
        template: src=templates/slaves.j2 dest=/usr/local/spark/conf/slaves 

      - name: deploy spark-env.sh configuration
        template: src=templates/spark-env.sh.j2 dest=/usr/local/spark/conf/spark-env.sh owner={{ hadoop_user }} group=hadoop
      
      tags: 
        - configuration        

# --------------------------------------------------
# Start the hadoop master
# --------------------------------------------------
- hosts: spark_masters

  vars_files:
    - vars/main.yml
  become: yes
  become_user: hadoop

  tasks:

    - block:
      - name: format hdfs (unless it's already been done)
        command: /usr/local/hadoop/bin/hadoop namenode -format creates=/usr/local/hadoop/ansible-format-hdfs
   
      - name: touch hfds formatted file (indicates if hdfs has been formatted)
        file: state=touch path=/usr/local/hadoop/ansible-format-hdfs

      - name: stop hadoop (if running)
        command: /usr/local/hadoop/sbin/stop-dfs.sh

      - name: start hadoop
        command: /usr/local/hadoop/sbin/start-dfs.sh

      tags: 
        - start-service
# --------------------------------------------------
# Kick of spark (making the master start the slaves)
# and configure spark-master as hadoop master
# --------------------------------------------------
- hosts: spark_masters
  
  vars_files:
    - vars/main.yml
  
  become: yes
  become_user: hadoop
  
  tasks:

    - block:
      - name: stop spark master (if running)
        command: /usr/local/spark/sbin/stop-master.sh

      - name: start spark master
        shell: SPARK_MASTER_IP="{{ ansible_hostname }}" /usr/local/spark/sbin/start-master.sh

      - name: stop the slaves (if running)
        shell: /usr/local/spark/sbin/stop-slaves.sh

      - name: start the slaves
        shell: /usr/local/spark/sbin/start-slaves.sh

      tags: 
        - start-service

