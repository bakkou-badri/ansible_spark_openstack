---
#
# Install basic packages and python
#
- name: install basic packages, python, and thunder
  hosts: all
  become: yes
  become_user: root
  #environment: 
  #  PATH: "/usr/local/miniconda/bin:{{ (ansible_env|default({})).PATH|default('') }}"

  vars:
    ssh_public_key_content: "{{ lookup('file', ssh_public_key ) }}"
   
  pre_tasks:
    - name: Update APT cache
      apt: update_cache=yes

  roles: 
    - common
    - {role: supervisord, tags: supervisor}
    - {role: miniconda, tags: miniconda}
    - {role: thunder, tags: thunder}

    # - {role: hadoop, tags: hadoop}
    # - {role: spark, tags: spark, environment: {PATH: "/usr/local/miniconda/bin:{{ ansible_env.PATH }}"}}

# # #
# # # Python installation and dependencies
# # #
# # - name: configure python
# #   hosts: all
# #   become: yes
# #   become_user: root
# #   environment:
#     PATH: "/usr/local/miniconda/bin:{{ ansible_env.PATH }}"

#   roles: 
#     - {role: miniconda, tags: miniconda}
#     - {role: thunder, tags: thunder}
#   tags: python

#
# Starting up spark, hadoop, and jupyterhub
# 
# Note: we do this after the python installation because we want to 
# use the custom python installed above instead of system python
#
- name: install and deploy spark and hdfs
  hosts: all
  become: yes
  become_user: hadoop
  environment:
    PATH: "/usr/local/miniconda/bin:{{ ansible_env.PATH }}"
    PYSPARK_PYTHON: "/usr/local/miniconda/bin/python"
  
  roles:
    - {role: spark, tags: spark}   
#    - {role: hadoop_master, tags: ['hadoop', 'start-hadoop']}
#    - {role: spark_master, tags: ['spark', 'start-spark']}
#    - {role: jupyterhub, tags: jupyterhub, become: yes, become_user: root, environment: {PATH: "/usr/local/miniconda/envs/python35/bin:{{ ansible_env.PATH }}"}}
